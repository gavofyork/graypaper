\section{Erasure Coding}\label{sec:erasurecoding}

\newcommand{\join}{\text{join}}
\newcommand{\spl}{\text{split}}

The foundation of the data-availability and distribution system of \Jam is a systematic Reed-Solomon erasure coding function in \textsc{gf}(16) of rate 342:1023, the same transform as done by the algorithm of \cite{lin2014novel}. We use a little-endian $\Y_2$ form of the 16-bit \textsc{gf} points with a functional equivalence given by $\se_2$. From this we may assume the encoding function $\mathcal{C}: \seq{\Y_2}_{342} \to \seq{\Y_2}_{1023}$ and the recovery function $\mathcal{R}: \powset[342]{\tuple{\Y_2, \N_{1023}}} \to \seq{\Y_2}_{342}$. Encoding is done by extrapolating a data blob of size 684 octets (provided in $\mathcal{C}$ here as 342 octet pairs) into 1,023 octet pairs. Recovery is done by collecting together any distinct 342 octet pairs, together with their indices, and transforming this into the original sequence of 342 octet pairs.

Practically speaking, this allows for the efficient encoding and recovery of data whose size is a multiple of 684 octets. Data whose length is not divisible by 684 must be padded (we pad with zeroes). We use this erasure-coding in two contexts within the \Jam protocol; one where we encode variable sized (but typically very large) data blobs for the Audit DA and block-distribution system, and the other where we encode much smaller fixed-size data \emph{segments} for the Import DA system.

For the Import DA system, we deal with an input size of 4,104 octets resulting in data-parallelism of order six. We may attain a greater degree of data parallelism if encoding or recovering more than one segment at a time though for recovery, we may be restricted to requiring each segment to be formed from the same set of indices (depending on the specific algorithm).

\subsection{Blob Encoding and Recovery}

\newcommand*{\unzip}{\text{unzip}}
\newcommand*{\lace}{\text{lace}}

We assume some data blob $\mathbf{d} \in \Y_{684k}, k \in \N$. We are able to express this as a whole number of $k$ pieces each of a sequence of 684 octets. We denote these (data-parallel) pieces $\mathbf{p} \in \seq{\Y_{684}} = \unzip_{684}(\mathbf{p})$. Each piece is then reformed as 342 octet pairs and erasure-coded using $\mathcal{C}$ as above to give 1,023 octet pairs per piece.

The resulting matrix is grouped by its pair-index and concatenated to form 1,023 \emph{chunks}, each of $k$ octet-pairs. Any 342 of these chunks may then be used to reconstruct the original data $\mathbf{d}$.

Formally we begin by defining four utility functions for splitting some large sequence into a number of equal-sized sub-sequences and for reconstituting such subsequences back into a single large sequence:
\begin{align}
  \forall n \in \N, k \in \N :\ &\spl_n(\mathbf{d} \in \Y_{k\cdot n}) \in \seq{\Y_n}_k \equiv \sq{\mathbf{d}_{0\dots+n}, \mathbf{d}_{n\dots+n}, \cdots, \mathbf{d}_{(k-1)n\dots+n}} \\
  \forall n \in \N, k \in \N :\ &\join_n(\mathbf{c} \in \seq{\Y_n}_k) \in \Y_{k\cdot n} \equiv \mathbf{c}_0 \concat \mathbf{c}_1 \concat \dots \\
  \forall n \in \N, k \in \N :\ &\unzip_n(\mathbf{d} \in \Y_{k\cdot n}) \in \seq{\Y_n}_k \equiv \sq{ [\mathbf{d}_{j.k + i} \mid j \in \N_n] \mid i \in \N_k} \\
  \forall n \in \N, k \in \N :\ &\lace_n(\mathbf{c} \in \seq{\Y_n}_k) \in \Y_{k\cdot n} \equiv \mathbf{d} \ \where \forall i \in \N_k, j \in \N_n: \mathbf{d}_{j.k + i} = (\mathbf{c}_i)_j
\end{align}

We define the transposition operator hence:
\begin{equation}\label{eq:transpose}
  {}^\text{T}[[\mathbf{x}_{0, 0}, \mathbf{x}_{0, 1}, \mathbf{x}_{0, 2}, \dots], [\mathbf{x}_{1, 0}, \mathbf{x}_{1, 1}, \dots], \dots] \equiv [[\mathbf{x}_{0, 0}, \mathbf{x}_{1, 0}, \mathbf{x}_{2, 0}, \dots], [\mathbf{x}_{0, 1}, \mathbf{x}_{1, 1}, \dots], \dots]
\end{equation}

We may then define our erasure-code chunking function which accepts an arbitrary sized data blob whose length divides wholly into 684 octets and results in 1,023 sequences of sequences each of smaller blobs:
\begin{equation}\label{eq:erasurecoding}
  \mathcal{C}_{k \in \N}\colon\left\{\begin{aligned}
    \Y_{684k} &\to \seq{\Y_{2k}}_{1023} \\
    \mathbf{d} &\mapsto [ \join(\mathbf{c}) \mid \mathbf{c} \orderedin {}^{\text{T}}[\mathcal{C}(\mathbf{p}) \mid \mathbf{p} \orderedin \text{unzip}_{684}(\mathbf{d})] ]
  \end{aligned}\right.
\end{equation}

The original data may be reconstructed with any 342 of the 1,023 resultant items (along with their indices). If the original 342 items are known then reconstruction is just their concatenation.
\begin{equation}\label{eq:erasurecodinginv}
  \mathcal{R}_{k \in \N}\colon\left\{\begin{aligned}
    \{(\Y_{2k}, \N_{1023})\}_{342} &\to \Y_{684k} \\
    \mathbf{c} &\mapsto \begin{cases}
      \se([\mathbf{x} \mid (\mathbf{x}, i) \orderedin \mathbf{c}]) &\when [i \mid (\mathbf{x}, i) \orderedin \mathbf{c}] = [0, 1, \dots 341]\\
      \lace_k([
        \mathcal{R}([(\spl_2(\mathbf{x})_p, i) \mid (\mathbf{x}, i) \orderedin \mathbf{c}])
      \mid p \in \N_k &\text{always}\\
    \end{cases}
    ])
%      [ \mathcal{R}(\mathbf{y}, i) \mid \mathbf{y} \orderedin \transpose[ \spl_2(\mathbf{x}) \mid (\mathbf{x}, i) \orderedin \mathbf{c}] ]
  \end{aligned}\right.
\end{equation}



Segment encoding/decoding may be done using the same functions albeit with a constant $k = 6$.

\subsection{Code Word representation}

For the sake of brevity we call each octet pair a \emph{word}. The code words (including the message words) are treated as element of $\mathbb{F}_{2^{16}}$ finite field. The field is generated as an extension of $\mathbb{F}_2$ using the irreducible polynomial:
\begin{equation}
x^{16} + x^5 + x^3 + x^2 + 1
\end{equation}

Hence:
\begin{equation}
\mathbb{F}_{16} \equiv \frac{\mathbb{F}_2[x]}{x^{16}} + x^5 + x^3 + x^2 + 1
\end{equation}

We name the generator of $\frac{\mathbb{F}_{16}}{\mathbb{F}_2}$, the root of the above polynomial, $\alpha$ as such: $\mathbb{F}_{16} = \mathbb{F}_2(\alpha)$.

Instead of using the standard basis $\{1, \alpha, \alpha^2, \dots, \alpha^{15}\}$, we opt for a representation of $\mathbb{F}_{16}$ which performs more efficiently for the encoding and the decoding process. To that aim, we name this specific representation of $\mathbb{F}_{16}$ as $\tilde{\mathbb{F}}_{16}$ and define it as a vector space generated by the following Cantor basis:

\begin{center}
  \begin{tabular}{ll}
    & \\
    \hline
    $v_0$ & $1$\\
    $v_1$ & $\alpha^{15} + \alpha^{13} + \alpha^{11} + \alpha^{10} + \alpha^7
    + \alpha^6 + \alpha^3 + \alpha$\\
    $v_2$ & $\alpha^{13} + \alpha^{12} + \alpha^{11} + \alpha^{10} + \alpha^3
    + \alpha^2 + \alpha$\\
    $v_3$ & $\alpha^{12} + \alpha^{10} + \alpha^9 + \alpha^5 + \alpha^4 +
    \alpha^3 + \alpha^2 + \alpha$\\
    $v_4$ & $\alpha^{15} + \alpha^{14} + \alpha^{10} + \alpha^8 + \alpha^7 +
    \alpha$\\
    $v_5$ & $\alpha^{15} + \alpha^{14} + \alpha^{13} + \alpha^{11} +
    \alpha^{10} + \alpha^8 + \alpha^5 + \alpha^3 + \alpha^2 + \alpha$\\
    $v_6$ & $\alpha^{15} + \alpha^{12} + \alpha^8 + \alpha^6 + \alpha^3 +
    \alpha^2$\\
    $v_7$ & $\alpha^{14} + \alpha^4 + \alpha$\\
    $v_8$ & $\alpha^{14} + \alpha^{13} + \alpha^{11} + \alpha^{10} + \alpha^7
    + \alpha^4 + \alpha^3$\\
    $v_9$ & $\alpha^{12} + \alpha^7 + \alpha^6 + \alpha^4 + \alpha^3$\\
    $v_{10}$ & $\alpha^{14} + \alpha^{13} + \alpha^{11} + \alpha^9 + \alpha^6
    + \alpha^5 + \alpha^4 + \alpha$\\
    $v_{11}$ & $\alpha^{15} + \alpha^{13} + \alpha^{12} + \alpha^{11} + \alpha^8$\\
    $v_{12}$ & $\alpha^{15} + \alpha^{14} + \alpha^{13} + \alpha^{12} + \alpha^{11} + \alpha^{10} + \alpha^8 + \alpha^7 + \alpha^5 + \alpha^4 + \alpha^3$\\
    $v_{13}$ & $\alpha^{15} + \alpha^{14} + \alpha^{13} + \alpha^{12} +
    \alpha^{11} + \alpha^9 + \alpha^8 + \alpha^5 + \alpha^4 + \alpha^2$\\
    $v_{14}$ & $\alpha^{15} + \alpha^{14} + \alpha^{13} + \alpha^{12} +
    \alpha^{11} + \alpha^{10} + \alpha^9 + \alpha^8 + \alpha^5 + \alpha^4 +
    \alpha^3$\\
    $v_{15}$ & $\alpha^{15} + \alpha^{12} + \alpha^{11} + \alpha^8 + \alpha^4
    + \alpha^3 + \alpha^2 + \alpha$\\
    \hline
  \end{tabular}
\end{center}

Every message word $m_i=m_{i, 15} \ldots m_{i, 0}$ consists of 16 bits. As such it could be regarded as binary vector of length 16:
\begin{equation}
m_i = (m_{i, 0} \ldots m_{i, 15})
\end{equation}

Where $m_{i, 0}$ is the least significant bit of message word $m_i$. Accordingly we consider the field element $\tilde{m}_i = \sum^{15}_{j = 0} m_{i, j} v_j$ to represent that message word.

Similarly, we assign a unique index to each validator between 0 and 1,022 and we represent validator $i$ with the field element:
\begin{equation}
\tilde{i} = \sum^{15}_{j = 0} i_j v_j
\end{equation}

where $i = i_{15} \ldots i_0$ is the binary representation of $i$.

\subsection{The Generator Polynomial}

To erasure code a message of 342 words into 1023 code words, we represent each message as a field element as described in previous section and we interpolate the polynomial $p(y)$ of maximum 341 degree which satisfies the following equalities:
\begin{equation}
   \begin{array}{l}
     p (\tilde{0}) = \widetilde{m_0}\\
     p (\tilde{1}) = \widetilde{m_1}\\
     \vdots\\
     p (\widetilde{341}) = \widetilde{m_{341}}
   \end{array}
\end{equation}

After finding $p(y)$ with such properties, we evaluate $p$ at the following points:
\begin{equation}
   \begin{array}{l}
     \widetilde{r_{342}} : = p (\widetilde{342})\\
     \widetilde{r_{343}} : = p (\widetilde{343})\\
     \vdots\\
     \widetilde{r_{1022}} : = p (\widetilde{1022})
   \end{array}
\end{equation}

We then distribute the message words and the extra code words among the validators according to their corresponding indices.

%To ensure the availability of work-package data in an adversarial environment, we use a Reed-Solomon code over the field $GF(2^{16})$ with rate of three to get a three times redundant data to be distributed among validators. This ensures that one-third of the validators may be dishonest or malfunctioning and yet any party is still able to reconstruct a work-package once we are assured that two-thirds-plus-one of the validators have each received their set of chunks for said work-package.

%A guarantor is responsible for making such an encoding for any work-package which they report and to ensure it is distributed among each validator. Further, validators 

%Essentially, we define a function:

%Define Reed-Soloman, the base, the 341 and the 1024.

%We divide the WP $\mathbf{p}$ into $c$ chunks of $n*2$ bytes where $c = \ceil{|p| / n*2}$ bytes (appending with zeroes as needed for the last chunk), and $c$ chunks each encoded of size $2*3n$ bytes, with each chunk requiring $n*2$ bytes to reconstruct and thus a total of $2cn$ to reconstruct $p$, assuming $2n$ bytes for each decoded chunk (\ie there is correspondance).

%We would send $3 * 2$ bytes to every validator (with $3 * 2$ bytes unsent). Then $1023/3+1$ validators could give us $(1023/3+1)*3*2$ bytes = 1023* 2 bytes, which is enough to reconstruct. In practice of course we'll batch this a lot to divide the $3*(unencoded size)$ data into 1024 chunks, 1 of which we send to each validator.

%The work-report should contain the root of a Merkle tree of these chunks. The guarantors (or validators sending chunks to auditors for recosnstruction) should send the Merkle proofs along with the chunks to each validator, which identifies that this chunk comes from this WP for availability voting/reconstruction. An auditor should encode the WP after obtaining it and recontruct this Merkle root to verify it. (Otherwise, it is possible that another auditor would not be able to reconstruct the same WP from $(1023/3+1)$ chunks with valid Merkle proofs, even if the first auditor reconstructed a WP with a matching WP hash from the report).
