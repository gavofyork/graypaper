\section{Discussion}\label{sec:discussion}


%\subsection{Security Assumptions}\label{sec:security}
%$\ge \twothirds$ validators honest and live.
%Short finality stall possible (up to one hour, more likely 12-18 seconds) where reversion possible. Such reversion results in major punishment of at least two validators. No reversion possible after finality.

%\subsection{Parachains and Liveness}

\subsection{Technical Characteristics}

In total, with our stated target of 1,023 validators and three validators per core, along with requiring a mean of ten audits per validator per timeslot, and thus 30 audits per work-report, \Jam is capable of trustlessly processing and integrating 341 work-packages per timeslot.

We assume node hardware is a modern 16 core \textsc{cpu} with 64\textsc{gb} \textsc{ram}, 8\textsc{tb} secondary storage and 0.5\textsc{g}be networking.

Our performance models assume a rough split of \textsc{cpu} time as follows:

\begin{center}
  \begin{tabular}[h]{@{}lll@{}}
    \toprule
  %  & \multicolumn{2}{c}{\textbf{Compute} / GB6} & \multicolumn{2}{c}{\textbf{Bandwidth} / $\text{KBs}^{-1}$} \\
    & \emph{Proportion} \\
    \midrule
    Audits & $\nicefrac{10}{16}$ \\
    Merklization & $\nicefrac{1}{16}$ \\
    Block execution & $\nicefrac{2}{16}$ \\
    \textsc{Grandpa} and \textsc{Beefy} & $\nicefrac{1}{16}$ \\
    Erasure coding & $\nicefrac{1}{16}$ \\
    Networking \& misc & $\nicefrac{1}{16}$ \\
    \bottomrule
  \end{tabular}
\end{center}

Estimates for network bandwidth requirements are as follows:
\begin{center}
\begin{tabular}[h]{@{}lll@{}}
  \toprule
%  & \multicolumn{2}{c}{\textbf{Compute} / GB6} & \multicolumn{2}{c}{\textbf{Bandwidth} / $\text{KBs}^{-1}$} \\
  Throughput, \textsc{mb}/slot & \emph{Tx} & \emph{Rx} \\
  \midrule
  Guaranteeing & 106 & 48 \\
  Assuring & 144 & 13 \\
  Auditing & 0 & 133 \\
  Authoring & 53 & 87 \\
  \textsc{Grandpa} and \textsc{Beefy} & 4 & 4 \\
  \textbf{Total} & \textbf{304} & \textbf{281} \\
  \midrule
  \textbf{Implied bandwidth}, \textsc{m}b/s & \textbf{387} & \textbf{357} \\
  \bottomrule
\end{tabular}
\end{center}

%Guarantor work-package acceptance and report publication represents a mean of 24\textsc{m}b/s download and 16\textsc{m}b/s upload. Guarantor/availability provision represents a mean of 12\textsc{m}b/s of network upload and 8\textsc{m}b/s network download bandwidth. Guarantor/auditing provision adds a further 80\textsc{m}b/s both upload and download in the regular case. Block publication/distribution represents a at most 42\textsc{m}b/s of network upload and download (but in general half of that and download-only), though this is bursty. \textsc{Grandpa} and \textsc{Beefy} may account for a further 4\textsc{m}b/s upload and download. The total mean network bandwidth requirements should therefore be around 158\textsc{m}b/s download and 154\textsc{m}b/s upload.
Thus, a connection able to sustain 500\textsc{m}b/s should leave a sufficient margin of error and headroom to serve other validators as well as some public connections, though the burstiness of block publication would imply validators are best to ensure that peak bandwidth is higher.

Under these conditions, we would expect an overall network-provided data availability capacity of 2\textsc{pb}, with each node dedicating at most $6$\textsc{tb} to availability storage.

Estimates for memory usage are as follows:

\begin{center}
  \begin{tabular}[h]{@{}lll@{}}
    \toprule
    & \textsc{gb} \\
    \midrule
    Auditing & 20 & 2 $\times$ 10 \textsc{pvm} instances \\
    Block execution & 2 & 1 \textsc{pvm} instance \\
    State cache & 40 & \\
    Misc & 2 & \\
    \textbf{Total} & \textbf{64} & \\
    \bottomrule
  \end{tabular}
\end{center}

%With 11 concurrent \textsc{pvm} instances each using an average of at most 2\textsc{gb} then we would expect 42\textsc{gb} left over for caching the state. Assuming 2\textsc{gb} for node operation, networking and ancillary databases, then 40\textsc{gb} is remaining for caching the most recent state in \textsc{ram}.
As a rough guide, each parachain has an average footprint of around 2\textsc{mb} in the Polkadot Relay chain; a 40\textsc{gb} state would allow 20,000 parachains' information to be retained in state.

What might be called the ``virtual hardware'' of a \Jam core is essentially a regular \textsc{cpu} core executing at somewhere between 25\% and 50\% of regular speed for the whole six-second portion and which may draw and provide 2\textsc{mb}/s average in general-purpose \textsc{i/o} and utilize up to 2\textsc{gb} in \textsc{ram}. The \textsc{i/o} includes any trustless reads from the \Jam chain state, albeit in the recent past. This virtual hardware also provides unlimited reads from a semi-static preimage-lookup database.

Each work-package may occupy this hardware and execute arbitrary code on it in six-second segments to create some result of at most 48\textsc{kb}. This work-result is then entitled to 10ms on the same machine, this time with no ``external'' \textsc{i/o}, but instead with full and immediate access to the \Jam chain state and may alter the service(s) to which the results belong.

\subsection{Illustrating Performance}

In terms of pure processing power, the \Jam machine architecture can deliver extremely high levels of homogeneous trustless computation. However, the core model of \Jam is a classic parallelized compute architecture, and for solutions to be able to utilize the architecture well they must be designed with it in mind to some extent. Accordingly, until such use-cases appear on \Jam with similar semantics to existing ones, it is very difficult to make direct comparisons to existing systems. That said, if we indulge ourselves with some assumptions then we can make some crude comparisons.

\subsubsection{Comparison to Polkadot}
Polkadot is at present capable of validating at most 80 parachains each doing one second of native computation and 5\textsc{mb} of \textsc{i/o} in every six. This corresponds to an aggregate compute performance of around 13x native \textsc{cpu} and a total 24-hour distributed availability of around 67\textsc{mb}/s. Accumulation is beyond Polkadot's capabilities and so not comparable.

For comparison, in our basic models, \Jam should be capable of attaining around 85x the computation load of a single native \textsc{cpu} core and a distributed availability of 682\textsc{mb}/s.

%---- Update from here.
\subsubsection{Simple Transfers}
We might also attempt to model a simple transactions-per-second amount, with each transaction requiring a signature verification and the modification of two account balances. Once again, until there are clear designs for precisely how this would work we must make some assumptions. Our most naive model would be to use the \Jam cores (\ie refinement) simply for transaction verification and account lookups. The \Jam chain would then hold and alter the balances in its state. This is unlikely to give great performance since almost all the needed \textsc{i/o} would be synchronous, but it can serve as a basis.

A 12\textsc{mb} work-package can hold around 96k transactions at 128 bytes per transaction. However, a 48\textsc{kb} work-result could only encode around 6k account updates when each update is given as a pair of a 4 byte account index and 4 byte balance, resulting in a limit of 3k transactions per package, or 171k \textsc{tps} in total. It is possible that the eight bytes could typically be compressed by a byte or two, increasing maximum throughput a little. Our expectations are that state updates, with highly parallelized Merklization, can be done at between 500k and 1 million reads/write per second, implying around 250k-350k \textsc{tps}, depending on which turns out to be the bottleneck.

A more sophisticated model would be to use the \Jam cores for balance updates as well as transaction verification. We would have to assume that state and the transactions which operate on them can be partitioned between work-packages with some degree of efficiency, and that the 12\textsc{mb} of the work-package would be split between transaction data and state witness data. Our basic models predict that a 32-bit account system paginated into $2^{10}$ accounts/page and 128 bytes per transaction could, assuming only around 1\% of oraclized accounts were useful, average upwards of 1.4m\textsc{tps} depending on partitioning and usage characteristics. Partitioning could be done with a fixed fragmentation (essentially sharding state), a rotating partition pattern or a dynamic partitioning (which would require specialized sequencing).

Interestingly, we expect neither model to be bottlenecked in computation, meaning that transactions could be substantially more sophisticated, perhaps with more flexible cryptography or smart-contract functionality, without a significant impact on performance.

\subsubsection{Computation Throughput}
The \textsc{tps} metric does not lend itself well to measuring distributed systems' computational performance, so we now turn to another slightly more compute-focussed benchmark: the \textsc{evm}. The basic \emph{YP} Ethereum network, now approaching a decade old, is probably the best known example of general purpose decentralized computation and makes for a reasonable yardstick. It is able to sustain a computation and \textsc{i/o} rate of 1.25M gas/sec, with a peak throughput of twice that. The \textsc{evm} gas metric was designed to be a time-proportional metric for predicting and constraining program execution. Attempting to determine a concrete comparison to \textsc{pvm} throughput is non-trivial and necessarily opinionated owing to the disparity between the two platforms, including word size, endianness, stack/register architecture and memory model. However, we will attempt to determine a reasonable range of values.

\textsc{Evm} gas does not directly translate into native execution as it also combines state reads and writes as well as transaction input data, implying it is able to process some combination of up to 595 storage reads, 57 storage writes and 1.25M computation-gas as well as 78\textsc{kb} input data in each second, trading one against the other.\footnote{The latest ``proto-danksharding'' changes allow it to accept 87.3\textsc{kb}/s in committed-to data though this is not directly available within state, so we exclude it from this illustration, though including it with the input data would change the results little.} We cannot find any analysis of the typical breakdown between storage \textsc{i/o} and pure computation, so to make a very conservative estimate, we assume it does all four. In reality, we would expect it to be able to do on average \nicefrac{1}{4} of each.

Our experiments\footnote{This is detailed at \url{{https://hackmd.io/@XXX9CM1uSSCWVNFRYaSB5g/HJarTUhJA}} and intended to be updated as we get more information.} show that on modern, high-end consumer hardware with a high-quality \textsc{evm} implementation, we can expect somewhere between 100 and 500 gas/µs in throughput on pure-compute workloads (we specifically utilized Odd-Product, Triangle-Number and several implementations of the Fibonacci calculation). To make a conservative comparison to \textsc{pvm}, we propose transpilation of the \textsc{evm} code into \textsc{pvm} code and then re-execution of it under the Polka\textsc{vm} prototype.\footnote{It is conservative since we don't take into account that the source code was originally compiled into \textsc{evm} code and thus the \textsc{pvm} machine code will replicate architectural artifacts and thus is very likely to be pessimistic. As an example, all arithmetic operations in \textsc{evm} are 256-bit and 64-bit native \textsc{pvm} is being forced to honor this even if the source code only actually required 64-bit values.}

To help estimate a reasonable lower-bound of \textsc{evm} gas/µs, \eg for workloads which are more memory and \textsc{i/o} intensive, we look toward real-world permissionless deployments of the \textsc{evm} and see that the Moonbeam network, after correcting for the slowdown of executing within the recompiled WebAssembly platform on the somewhat conservative Polkadot hardware platform, implies a throughput of around 100 gas/µs. We therefore assert that in terms of computation, 1µs approximates to around 100-500 \textsc{evm} gas on modern high-end consumer hardware.\footnote{We speculate that the substantial range could possibly be caused in part by the major architectural differences between the \textsc{evm} \textsc{isa} and typical modern hardware.}

Benchmarking and regression tests show that the prototype \textsc{pvm} engine has a fixed preprocessing overhead of around 5ns/byte of program code and, for arithmetic-heavy tasks at least, a marginal factor of 1.6-2\% compared to \textsc{evm} execution, implying an asymptotic speedup of around 50-60x. For machine code 1\textsc{mb} in size expected to take of the order of a second to compute, the compilation cost becomes only 0.5\% of the overall time. \footnote{As an example, our odd-product benchmark, a very much pure-compute arithmetic task, execution takes 58s on \textsc{evm}, and 1.04s within our \textsc{pvm} prototype, including all preprocessing.} For code not inherently suited to the 256-bit \textsc{evm} \textsc{isa}, we would expect substantially improved relative execution times on \textsc{pvm}, though more work must be done in order to gain confidence that these speed-ups are broadly applicable.

If we allow for preprocessing to take up to the same component within execution as the marginal cost (owing to, for example, an extremely large but short-running program) and for the \textsc{pvm} metering to imply a safety overhead of 2x to execution speeds, then we can expect a \Jam core to be able to process the equivalent of around 1,500 \textsc{evm} gas/µs. Owing to the crudeness of our analysis we might reasonably predict it to be somewhere within a factor of three either way---\ie 500-5,000 \textsc{evm} gas/µs.

\Jam cores are each capable of 2\textsc{mb}/s bandwidth, which must include any state \textsc{i/o} and data which must be newly introduced (\eg transactions). While writes come at comparatively little cost to the core, only requiring hashing to determine an eventual updated Merkle root, reads must be witnessed, with each one costing around 640 bytes of witness conservatively assuming a one-million entry binary Merkle trie. This would result in a maximum of a little over 3k reads/second/core, with the exact amount dependent upon how much of the bandwidth is used for newly introduced input data.

Aggregating everything across \Jam, excepting accumulation which could add further throughput, numbers can be multiplied by 341 (with the caveat that each one's computation cannot interfere with any of the others' except through state oraclization and accumulation). Unlike for \emph{roll-up chain} designs such as Polkadot and Ethereum, there is no need to have persistently fragmented state. Smart-contract state may be held in a coherent format on the \Jam chain so long as any updates are made through the 8\textsc{kb}/core/sec work-results, which would need to contain only the hashes of the altered contracts' state roots.

Under our modelling assumptions, we can therefore summarize:
\begin{center}
  \begin{tabular}[h]{@{}llll@{}}
    \toprule
    & Eth. L1 & \Jam Core & \Jam \\
    \midrule
    Compute (\textsc{evm} gas/µs) & $1.25^\dagger$ & 500-5,000 & 0.15-1.5\textsc{m} \\
    State writes (s$^{-1}$) & $57^\dagger$ & n/a & n/a \\
    State reads (s$^{-1}$) & $595^\dagger$ & 4\textsc{k}${}^\ddagger$ & 1.4\textsc{m}${}^\ddagger$ \\
    Input data (s$^{-1}$) & 78\textsc{kb}${}^\dagger$ & 2\textsc{mb}${}^\ddagger$ & 682\textsc{mb}${}^\ddagger$ \\
    \bottomrule
  \end{tabular}
\end{center}

What we can see is that \Jam's overall predicted performance profile implies it could be comparable to many thousands of that of the basic Ethereum L1 chain. The large factor here is essentially due to three things: spacial parallelism, as \Jam can host several hundred cores under its security apparatus; temporal parallelism, as \Jam targets continuous execution for its cores and pipelines much of the computation between blocks to ensure a constant, optimal workload; and platform optimization by using a \textsc{vm} and gas model which closely fits modern hardware architectures.

It must however be understood that this is a provisional and crude estimation only. It is included only for the purpose of expressing \Jam's performance in tangible terms. Specifically, it does not take into account:
\begin{itemize}
  \item that these numbers are based on real performance of Ethereum and performance modelling of \Jam (though our models are based on real-world performance of the components);
  \item any L2 scaling which may be possible with either \Jam or Ethereum;
  \item the state partitioning which uses of \Jam would imply;
  \item the as-yet unfixed gas model for the \textsc{pvm};
  \item that \textsc{pvm}/\textsc{evm} comparisons are necessarily imprecise;
  \item (${}^\dagger$) all figures for Ethereum L1 are drawn from the same resource: on average each figure will be only $\nicefrac{1}{4}$ of this maximum.
  \item (${}^\ddagger$) the state reads and input data figures for \Jam are drawn from the same resource: on average each figure will be only $\nicefrac{1}{2}$ of this maximum.
\end{itemize}

We leave it as further work for an empirical analysis of performance and an analysis and comparison between \Jam and the aggregate of a hypothetical Ethereum ecosystem which included some maximal amount of L2 deployments together with full Dank-sharding and any other additional consensus elements which they would require. This, however, is out of scope for the present work.

