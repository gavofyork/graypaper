\section{Previous Work and Present Trends}\label{sec:previouswork}

In the years since the initial publication of the Ethereum \emph{YP}, the field of blockchain development has grown immensely. Other than scalability, development has been done around underlying consensus algorithms, smart-contract languages and machines and overall state environments. While interesting, these latter subjects are mostly out scope of the present work since they generally do not impact underlying scalability.

\subsection{Polkadot}

In order to deliver its service, \Jam co-opts much of the same game-theoretic and cryptographic machinery as Polkadot known as \textsc{Elves} and described by \cite{cryptoeprint:2024/961}. However, major differences exist in the actual service offered with \Jam, providing an abstraction much closer to the actual computation model generated by the validator nodes its economy incentivizes.

It was a major point of the original Polkadot proposal, a scalable heterogeneous multichain, to deliver high-performance through partition and distribution of the workload over multiple host machines. In doing so it took an explicit position that composability would be lowered. Polkadot's constituent components, parachains are, practically speaking, highly isolated in their nature. Though a message passing system (\textsc{xcmp}) exists it is asynchronous, coarse-grained and practically limited by its reliance on a high-level slowly evolving interaction language \textsc{xcm}.

As such, the composability offered by Polkadot between its constituent chains is lower than that of Ethereum-like smart-contract systems offering a single and universal object environment and allowing for the kind of agile and innovative integration which underpins their success. Polkadot, as it stands, is a collection of independent ecosystems with only limited opportunity for collaboration, very similar in ergonomics to bridged blockchains though with a categorically different security profile. A technical proposal known as \textsc{spree} would utilize Polkadot's unique shared-security and improve composability, though blockchains would still remain isolated.

Implementing and launching a blockchain is hard, time-consuming and costly. By its original design, Polkadot limits the clients able to utilize its service to those who are both able to do this and raise a sufficient deposit to win an auction for a long-term slot, one of around 50 at the present time. While not permissioned per se, accessibility is categorically and substantially lower than for smart-contract systems similar to Ethereum.

Enabling as many innovators to participate and interact, both with each other and each other's user-base, appears to be an important component of success for a Web3 application platform. Accessibility is therefore crucial.

\subsection{Ethereum}

The Ethereum protocol was formally defined in this paper's spiritual predecessor, the \emph{Yellow Paper}, by \cite{wood2014ethereum}. This was derived in large part from the initial concept paper by \cite{buterin2013ethereum}. In the decade since the \emph{YP} was published, the \emph{de facto} Ethereum protocol and public network instance have gone through a number of evolutions, primarily structured around introducing flexibility via the transaction format and the instruction set and ``precompiles'' (niche, sophisticated bonus instructions) of its scripting core, the Ethereum virtual machine (\textsc{evm}).

Almost one million crypto-economic actors take part in the validation for Ethereum.\footnote{Practical matters do limit the level of real decentralization. Validator software expressly provides functionality to allow a single instance to be configured with multiple key sets, systematically facilitating a much lower level of actual decentralization than the apparent number of actors, both in terms of individual operators and hardware. Using data collated by \cite{hildobby2024eth2} on Ethereum 2, one can see one major node operator, Lido, has steadily accounted for almost one-third of the almost one million crypto-economic participants.} Block extension is done through a randomized leader-rotation method where the physical address of the leader is public in advance of their block production.\footnote{Ethereum's developers hope to change this to something more secure, but no timeline is fixed.} Ethereum uses Casper-\textsc{ffg} introduced by \cite{buterin2019casper} to determine finality, which with the large validator base finalizes the chain extension around every 13 minutes.

Ethereum's direct computational performance remains broadly similar to that with which it launched in 2015, with a notable exception that an additional service now allows 1\textsc{mb} of \emph{commitment data} to be hosted per block (all nodes to store it for a limited period). The data cannot be directly utilized by the main state-transition function, but special functions provide proof that the data (or some subsection thereof) is available. According to \cite{ethereum2024danksharding}, the present design direction is to improve on this over the coming years by splitting responsibility for its storage amongst the validator base in a protocol known as \emph{Dank-sharding}.

According to \cite{ethereum2024sigital}, the scaling strategy of Ethereum would be to couple this data availability with a private market of \emph{roll-ups}, sideband computation facilities of various design, with \textsc{zk-snark}-based roll-ups being a stated preference. Each vendor's roll-up design, execution and operation comes with its own implications.

One might reasonably assume that a diversified market-based approach for scaling via multivendor roll-ups will allow well-designed solutions to thrive. However, there are potential issues facing the strategy. A research report by \cite{sharma2024ethereums} on the level of decentralization in the various roll-ups found a broad pattern of centralization, but notes that work is underway to attempt to mitigate this. It remains to be seen how decentralized they can yet be made.

Heterogeneous communication properties (such as datagram latency and semantic range), security properties (such as the costs for reversion, corruption, stalling and censorship) and economic properties (the cost of accepting and processing some incoming message or transaction) may differ, potentially quite dramatically, between major areas of some grand patchwork of roll-ups by various competing vendors. While the overall Ethereum network may eventually provide some or even most of the underlying machinery needed to do the sideband computation it is far from clear that there would be a ``grand consolidation'' of the various properties should such a thing happen. We have not found any good discussion of the negative ramifications of such a fragmented approach.\footnote{Some initial thoughts on the matter resulted in a proposal by \cite{sadana2024bringing} to utilize Polkadot technology as a means of helping create a modicum of compatibility between roll-up ecosystems!}

\subsubsection{\textsc{Snark} Roll-ups}

While the protocol's foundation makes no great presuppositions on the nature of roll-ups, Ethereum's strategy for sideband computation does centre around \textsc{snark}-based rollups and as such the protocol is being evolved into a design that makes sense for this. \textsc{Snark}s are the product of an area of exotic cryptography which allow proofs to be constructed to demonstrate to a neutral observer that the purported result of performing some predefined computation is correct. The complexity of the verification of these proofs tends to be sub-linear in their size of computation to be proven and will not give away any of the internals of said computation, nor any dependent witness data on which it may rely.

\textsc{Zk-snark}s come with constraints. There is a trade-off between the proof's size, verification complexity and the computational complexity of generating it. Non-trivial computation, and especially the sort of general-purpose computation laden with binary manipulation which makes smart-contracts so appealing, is hard to fit into the model of \textsc{snark}s.

To give a practical example, \textsc{risc}-zero (as assessed by \cite{bogli2024assessing}) is a leading project and provides a platform for producing \textsc{snark}s of computation done by a \textsc{risc-v} virtual machine, an open-source and succinct \textsc{risc} machine architecture well-supported by tooling. A recent benchmarking report by \cite{koute2024risc0} showed that compared to \textsc{risc}-zero's own benchmark, proof generation alone takes over 61,000 times as long as simply recompiling and executing even when executing on 32 times as many cores, using 20,000 times as much \textsc{ram} and an additional state-of-the-art \textsc{gpu}. According to hardware rental agents \url{https://cloud-gpus.com/}, the cost multiplier of proving using \textsc{risc}-zero is 66,000,000x of the cost\footnote{In all likelihood actually substantially more as this was using low-tier ``spare'' hardware in consumer units, and our recompiler was unoptimized.} to execute using the Polka\textsc{vm} recompiler.

Many cryptographic primitives become too expensive to be practical to use and specialized algorithms and structures must be substituted. Often times they are otherwise suboptimal. In expectation of the use of \textsc{snark}s (such as \textsc{plonk} as proposed by \cite{cryptoeprint:2019/953}), the prevailing design of the Ethereum project's Dank-sharding availability system uses a form of erasure coding centered around polynomial commitments over a large prime field in order to allow \textsc{snark}s to get acceptably performant access to subsections of data. Compared to alternatives, such as a binary field and Merklization in the present work, it leads to a load on the validator nodes orders of magnitude higher in terms of \textsc{cpu} usage.

In addition to their basic cost, \textsc{snark}s present no great escape from decentralization and the need for redundancy, leading to further cost multiples. While the need for some benefits of staked decentralization is averted through their verifiable nature, the need to incentivize multiple parties to do much the same work is a requirement to ensure that a single party not form a monopoly (or several not form a cartel). Proving an incorrect state-transition should be impossible, however service integrity may be compromised in other ways; a temporary suspension of proof-generation, even if only for minutes, could amount to major economic ramifications for real-time financial applications.

Real-world examples exist of the pit of centralization giving rise to monopolies. One would be the aforementioned \textsc{snark}-based exchange framework; while notionally serving decentralized exchanges, it is in fact centralized with Starkware itself wielding a monopoly over enacting trades through the generation and submission of proofs, leading to a single point of failure---should Starkware's service become compromised, then the liveness of the system would suffer.

It has yet to be demonstrated that \textsc{snark}-based strategies for eliminating the trust from computation will ever be able to compete on a cost-basis with a multi-party crypto-economic platform. All as-yet proposed \textsc{snark}-based solutions are heavily reliant on crypto-economic systems to frame them and work around their issues. Data availability and sequencing are two areas well understood as requiring a crypto-economic solution.

We would note that \textsc{snark} technology is improving and the cryptographers and engineers behind them do expect improvements in the coming years. In a recent article by \cite{thaler2023technical} we see some credible speculation that with some recent advancements in cryptographic techniques, slowdowns for proof generation could be as little as 50,000x from regular native execution and much of this could be parallelized. This is substantially better than the present situation, but still several orders of magnitude greater than would be required to compete on a cost-basis with established crypto-economic techniques such as \textsc{Elves}.

\subsection{Fragmented Meta-Networks}

Directions for general-purpose computation scalability taken by other projects broadly centre around one of two approaches; either what might be termed a \emph{fragmentation} approach or alternatively a \emph{centralization} approach. We argue that neither approach offers a compelling solution.

The fragmentation approach is heralded by projects such as Cosmos (proposed by \cite{kwon2019cosmos}) and Avalanche (by \cite{tanana2019avalanche}). It involves a system fragmented by networks of a homogenous consensus mechanic, yet staffed by separately motivated sets of validators. This is in contrast to Polkadot's single validator set and Ethereum's declared strategy of heterogeneous roll-ups secured partially by the same validator set operating under a coherent incentive framework. The homogeneity of said fragmentation approach allows for reasonably consistent messaging mechanics, helping to present a fairly unified interface to the multitude of connected networks.

However, the apparent consistency is superficial. The networks are trustless only by assuming correct operation of their validators, who operate under a crypto-economic security framework ultimately conjured and enforced by economic incentives and punishments. To do twice as much work with the same levels of security and no special coordination between validator sets, then such systems essentially prescribe forming a new network with the same overall levels of incentivization.

Several problems arise. Firstly, there is a similar downside as with Polkadot's isolated parachains and Ethereum's isolated roll-up chains: a lack of coherency due to a persistently sharded state preventing synchronous composability.

More problematically, the scaling-by-fragmentation approach, proposed specifically by Cosmos, provides no homogenous security---and therefore trustlessness---guarantees. Validator sets between networks must be assumed to be independently selected and incentivized with no relationship, causal or probabilistic, between the Byzantine actions of a party on one network and potential for appropriate repercussions on another. Essentially, this means that should validators conspire to corrupt or revert the state of one network, the effects may be felt across other networks of the ecosystem.

That this is an issue is broadly accepted, and projects propose for it to be addressed in one of two ways. Firstly, to fix the expected cost-of-attack (and thus level of security) across networks by drawing from the same validator set. The massively redundant way of doing this, as proposed by \cite{cosmos2024interchain} under the name \emph{replicated security}, would be to require each validator to validate on all networks and for the same incentives and punishments. This is economically inefficient in the cost of security provision as each network would need to independently provide the same level of incentives and punishment-requirements as the most secure with which it wanted to interoperate. This is to ensure the economic proposition remain unchanged for validators and the security proposition remained equivalent for all networks. At the present time, replicated security is not a readily available permissionless service. We might speculate that these punishing economics have something to do with it.

The more efficient approach, proposed by the OmniLedger team, \cite{cryptoeprint:2017/406}, would be to make the validators non-redundant, partitioning them between different networks and periodically, securely and randomly repartitioning them. A reduction in the cost to attack over having them all validate on a single network is implied since there is a chance of having a single network accidentally have a compromising number of malicious validators even with less than this proportion overall. This aside it presents an effective means of scaling under a basis of weak-coherency.

Alternatively, as in \textsc{Elves} by \cite{cryptoeprint:2024/961}, we may utilize non-redundant partitioning, combine this with a proposal-and-auditing game which validators play to weed out and punish invalid computations, and then require that the finality of one network be contingent on all causally-entangled networks. This is the most secure and economically efficient solution of the three, since there is a mechanism for being highly confident that invalid transitions will be recognized and corrected before their effect is finalized across the ecosystem of networks. However, it requires substantially more sophisticated logic and their causal-entanglement implies some upper limit on the number of networks which may be added.

\subsection{High-Performance Fully Synchronous Networks}

Another trend in the recent years of blockchain development has been to make ``tactical'' optimizations over data throughput by limiting the validator set size or diversity, focusing on software optimizations, requiring a higher degree of coherency between validators, onerous requirements on the hardware which validators must have, or limiting data availability.

The Solana blockchain is underpinned by technology introduced by \cite{yakovenko2018solana} and boasts theoretical figures of over 700,000 transactions per second, though according to \cite{ng2024is} the network is only seen processing a small fraction of this. The underlying throughput is still substantially more than most blockchain networks and is owed to various engineering optimizations in favor of maximizing synchronous performance. The result is a highly-coherent smart-contract environment with an \textsc{api} not unlike that of \emph{YP} Ethereum (albeit using a different underlying \textsc{vm}), but with a near-instant time to inclusion and finality which is taken to be immediate upon inclusion.

Two issues arise with such an approach: firstly, defining the protocol as the outcome of a heavily optimized codebase creates structural centralization and can undermine resilience. \cite{jha2024solana} writes ``since January 2022, 11 significant outages gave rise to 15 days in which major or partial outages were experienced''. This is an outlier within the major blockchains as the vast majority of major chains have no downtime. There are various causes to this downtime, but they are generally due to bugs found in various subsystems.

Ethereum, at least until recently, provided the most contrasting alternative with its well-reviewed specification, clear research over its crypto-economic foundations and multiple clean-room implementations. It is perhaps no surprise that the network very notably continued largely unabated when a flaw in its most deployed implementation was found and maliciously exploited, as described by \cite{hertig2016so}.

The second issue is concerning ultimate scalability of the protocol when it provides no means of distributing workload beyond the hardware of a single machine.

In major usage, both historical transaction data and state would grow impractically. Solana illustrates how much of a problem this can be. Unlike classical blockchains, the Solana protocol offers no solution for the archival and subsequent review of historical data, crucial if the present state is to be proven correct from first principle by a third party. There is little information on how Solana manages this in the literature, but according to \cite{solana2023solana}, nodes simply place the data onto a centralized database hosted by Google.\footnote{Earlier node versions utilized Arweave network, a decentralized data store, but this was found to be unreliable for the data throughput which Solana required.}

Solana validators are encouraged to install large amounts of \textsc{ram} to help hold its large state in memory (512 \textsc{gb} is the current recommendation according to \cite{solana2024solana}). Without a divide-and-conquer approach, Solana shows that the level of hardware which validators can reasonably be expected to provide dictates the upper limit on the performance of a totally synchronous, coherent execution model. Hardware requirements represent barriers to entry for the validator set and cannot grow without sacrificing decentralization and, ultimately, transparency.
