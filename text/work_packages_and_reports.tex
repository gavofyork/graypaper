\section{Work Packages and Work Reports}\label{sec:workpackagesandworkreports}

\newcommand*{\newavailabilityspecifier}{A}
\newcommand*{\itemtoresult}{C}
\newcommand*{\countupexports}{I}
\newcommand*{\importsegmentdata}{M}
\newcommand*{\pagedproofs}{P}
\newcommand*{\marshallrefine}{R}
\newcommand*{\extrinsicdata}{X}
\newcommand*{\wpX}{p}
\newcommand*{\wiX}{w}

\subsection{Honest Behavior}

We have so far specified how to recognize blocks for a correctly transitioning \Jam blockchain. Through defining the state transition function and a state Merklization function, we have also defined how to recognize a valid header. While it is not especially difficult to understand how a new block may be authored for any node which controls a key which would allow the creation of the two signatures in the header, nor indeed to fill in the other header fields, readers will note that the contents of the extrinsic remain unclear.

We define not only correct behavior through the creation of correct blocks but also \emph{honest behavior}, which involves the node taking part in several \emph{off-chain} activities. This does have analogous aspects within \emph{YP} Ethereum, though it is not mentioned so explicitly in said document: the creation of blocks along with the gossiping and inclusion of transactions within those blocks would all count as off-chain activities for which honest behavior is helpful. In \Jam's case, honest behavior is well-defined and expected of at least $\nicefrac{2}{3}$ of validators.

Beyond the production of blocks, incentivized honest behavior includes:
\begin{itemize}
    \item the guaranteeing and reporting of work-packages, along with chunking and distribution of both the chunks and the work-package itself, discussed in section \ref{sec:guaranteeing};
    \item assuring the availability of work-packages after being in receipt of their data;
    \item determining which work-reports to audit, fetching and auditing them, and creating and distributing judgements appropriately based on the outcome of the audit;
    \item submitting the correct amount of auditing work seen being done by other validators, discussed in section \ref{sec:bookkeeping}.
\end{itemize}

\subsection{Segments and the Manifest}

Our basic erasure-coding segment size is $\mathsf{W}_C = 684$ octets, derived from the fact we wish to be able to reconstruct even should almost two-thirds of our 1023 participants be malicious or incapacitated, the 16-bit Galois field on which the erasure-code is based and the desire to efficiently support encoding data of close to, but no less than, 4\textsc{kb}.

Work-packages are generally small to ensure guarantors need not invest a lot of bandwidth in order to discover whether they can get paid for their evaluation into a work-report. Rather than having much data inline, they instead \emph{reference} data through commitments. The simplest commitments are extrinsic data.

Extrinsic data are blobs which are being introduced into the system alongside the work-package itself generally by the work-package builder. They are exposed to the Refine logic as an argument. We commit to them through including each of their hashes in the work-package.

Work-packages have two other types of external data associated with them: A cryptographic commitment to each \emph{imported} segment and finally the number of segments which are \emph{exported}.

\subsubsection{Segments, Imports and Exports}

The ability to communicate large amounts of data from one work-package to some subsequent work-package is a key feature of the \Jam availability system. An export segment, defined as the set $\G$, is an octet sequence of fixed length $\mathsf{W}_S\mathsf{W}_C = 4104$. It is the smallest datum which may individually be imported from---or exported to---the long-term \emph{Imports DA} during the Refine function of a work-package. Being an exact multiple of the erasure-coding piece size ensures that the data segments of work-package can be efficiently placed in the DA system.
\begin{equation}\label{eq:segment}
  \G \equiv \Y_{\mathsf{W}_S\mathsf{W}_C}
\end{equation}

Exported segments are data which are \emph{generated} through the execution of the Refine logic and thus are a side effect of transforming the work-package into a work-report. Since their data is deterministic based on the execution of the Refine logic, we do not require any particular commitment to them in the work-package beyond knowing how many are associated with each Refine invocation in order that we can supply an exact index.

On the other hand, imported segments are segments which were exported by previous work-packages. In order for them to be easily fetched and verified they are referenced not by hash but rather the root of a Merkle tree which includes any other segments introduced at the time, together with an index into this sequence. This allows for justifications of correctness to be generated, stored, included alongside the fetched data and verified. This is described in depth in the next section.

\subsubsection{Data Collection and Justification}

% TODO: Mention core affinity and guarantor-guarantor distribution.

It is the task of a guarantor to reconstitute all imported segments through fetching said segments' erasure-coded chunks from enough unique validators. Reconstitution alone is not enough since corruption of the data would occur if one or more validators provided an incorrect chunk. For this reason we ensure that the import segment specification (a Merkle root and an index into the tree) be a kind of cryptographic commitment capable of having a justification applied to demonstrate that any particular segment is indeed correct.

Justification data must be available to any node over the course of its segment's potential requirement. At around 350 bytes to justify a single segment, justification data is too voluminous to have all validators store all data. We therefore use the same overall availability framework for hosting justification metadata as the data itself.

The guarantor is able to use this proof to justify to themselves that they are not wasting their time on incorrect behavior. We do not force auditors to go through the same process. Instead, guarantors build an \emph{Auditable Work Package}, and place this in the Audit DA system. This is the original work-package, its extrinsic data, its imported data and a concise proof of correctness of that imported data. This tactic routinely duplicates data between the Imports DA and the Audits DA, however it is acceptable in order to reduce the bandwidth cost for auditors who must justify the correctness as cheaply as possible as auditing happens on average 30 times for each work-package whereas guaranteeing happens only twice or thrice.

\subsection{Packages and Items}\label{sec:packagesanditems}

We begin by defining a \emph{work-package}, of set $\mathbb{P}$, and its constituent \emph{work item}s, of set $\mathbb{I}$. A work-package includes a simple blob acting as an authorization token $\wp¬authtoken$, the index of the service which hosts the authorization code $\wp¬authcodehost$, an authorization code hash $\wp¬authcodehash$ and a parameterization blob $\wp¬authparam$, a context $\wp¬context$ and a sequence of work items $\wp¬workitems$:
\begin{equation}\label{eq:workpackage}
  \mathbb{P} \equiv \tuple{\;\begin{aligned}
    &\isa{\wp¬authtoken}{\Y},\ \isa{\wp¬authcodehost}{\N_S},\ \isa{\wp¬authcodehash}{\H},\\
    &\isa{\wp¬authparam}{\Y},\ \isa{\wp¬context}{\mathbb{X}},\ \isa{\wp¬workitems}{\seq{\mathbb{I}}_{1:\mathsf{I}}}
  \end{aligned}}
\end{equation}

A work item includes: $s$ the identifier of the service to which it relates, the code hash of the service at the time of reporting $c$ (whose preimage must be available from the perspective of the lookup anchor block), a payload blob $\mathbf{y}$, a gas limit $g$, and the three elements of its manifest, a sequence of imported data segments $\wi¬importsegments$ identified by the root of the \emph{segments tree} and an index into it, $\wi¬extrinsics$, a sequence of hashed of blob hashes and lengths to be introduced in this block (and which we assume the validator knows) and $\wi¬exportsegments$ the number of data segments exported by this work item:
\begin{equation}\label{eq:workitem}
    \mathbb{I} \equiv \tuple{\begin{aligned}
      &\isa{s}{\N_S} \ts
      \isa{c}{\H} \ts
      \isa{\mathbf{y}}{\Y} \ts
      \isa{g}{\N_G} \ts \\
      &\isa{\wi¬importsegments}{\seq{\tuple{\H,\N}}} \ts
      \isa{\wi¬extrinsics}{\seq{(\H, \N)}} \ts
      \isa{\wi¬exportsegments}{\N}
    \end{aligned}}
\end{equation}

We limit the total number of exported items to $\mathsf{W}_M = 2^{11}$. We also place the same limit on the total number of imported items:
\begin{align}
  \forall \wpX \in \mathbb{P}:
  \sum_{\wiX \in \wpX_\wp¬workitems} \wiX_\wi¬exportsegments &\le \mathsf{W}_M \ \wedge\ 
  \sum_{\wiX \in \wpX_\wp¬workitems} |\wiX_\wi¬importsegments| \le \mathsf{W}_M
\end{align}

We make an assumption that the preimage to each extrinsic hash in each work-item is known by the guarantor. In general this data will be passed to the guarantor alongside the work-package.

We limit the encoded size of a work-package plus the total size of the implied import and extrinsic items to 12\textsc{mb} in order to allow for around 2\textsc{mb}/s/core data throughput:
\begin{align}
  \label{eq:checkextractsize}
  \forall \wpX &\in \mathbb{P}: \left(
  \sum_{\wiX \in \wpX_\wp¬workitems} |\wiX_\wi¬importsegments|\cdot\mathsf{W}_S\mathsf{W}_C + \sum_{\wiX \in \wpX_\wp¬workitems} \sum_{(h, l) \in \wiX_\wi¬extrinsics} \!\!\!l\right) \le \mathsf{W}_P \\
  \mathsf{W}_P &= 12\cdot2^{20}
\end{align}

%The implication of equation \ref{eq:checkextractsize} is that we have access to the preimage of all extrinsic data. For guaranteeing, this implies the work-package author probably submits the preimages alongside the work-package itself. For auditing, the extrinsic preimages may be reconstructed in the same manner as the work-package. Both are described later.

Given the result $o$ of some work-item, we define the item-to-result function $\itemtoresult$ as:
\begin{equation}
  \itemtoresult\colon\left\{\begin{aligned}
    (\mathbb{I}, \Y \cup \mathbb{J}) &\to \mathbb{L}\\
    ((s, c, \mathbf{y}, g), o) &\mapsto (s, c, \mathcal{H}(\mathbf{y}), g, o)
  \end{aligned}\right.
\end{equation}

We define the work-package's implied authorizer as $\wpX_\wp¬authorizer$, the hash of the concatenation of the authorization code and the parameterization. We define the authorization code as $\wpX_\wp¬authcode$ and require that it be available at the time of the lookup anchor block from the historical lookup of service $\wpX_\wp¬authcodehost$. Formally:
\begin{equation}
  \forall \wpX \in \mathbb{P}: \left\{\,\begin{aligned}
    \wpX_\wp¬authorizer &\equiv \mathcal{H}(\wpX_\wp¬authcode \concat \wpX_\wp¬authparam) \\
    \wpX_\wp¬authcode &\equiv \Lambda(\delta[\wpX_\wp¬authcodehost], (\wpX_\wp¬context)_t, \wpX_\wp¬authcodehash) \\
    \wpX_\wp¬authcode &\in \Y
  \end{aligned}\right.
\end{equation}

(The historical lookup function, $\Lambda$, is defined in equation \ref{eq:historicallookup}.)

\subsubsection{Exporting}
Any of a work-package's work-items may \emph{export} segments and a \emph{segments-root} is placed in the work-report committing to these, ordered according to the work-item which is exporting. It is formed as the root of a constant-depth binary Merkle tree as defined in equation \ref{eq:constantdepthmerkleroot}.

Guarantors are required to erasure-code and distribute two data sets: one blob, the auditable work-package containing the encoded work-package, extrinsic data and self-justifying imported segments which is placed in the short-term Audit DA store and a second set of exported-segments data together with the \emph{Paged-Proofs} metadata. Items in the first store are short-lived; assurers are expected to keep them only until finality of the block which included the work-result. Items in the second, meanwhile, are long-lived and expected to be kept for a minimum of 28 days (672 complete epochs) following the reporting of the work-report.

We define the paged-proofs function $\pagedproofs$ which accepts a series of exported segments $\mathbf{s}$ and defines some series of additional segments placed into the Imports DA system via erasure-coding and distribution. The function evaluates to pages of hashes, together with subtree proofs, such that justifications of correctness based on a segments-root may be made from it:
\begin{equation}\label{eq:pagedproofs}
  \pagedproofs\colon\left\{\begin{aligned}
    \seq{\G} \to \,&\seq{\G} \\
    \mathbf{s} \mapsto \,&[
      \mathcal{P}_l(\se(
        \var{\mathcal{J}_6(\mathbf{s}, i)},
        \var{\mathbf{s}_{i\dots+64}}
      ))
      \mid i \orderedin 64\cdot\N_{\ceil{\nicefrac{|\mathbf{s}|}{64}}}
    ] \\
    & \where l = \mathsf{W}_S\mathsf{W}_C
  \end{aligned}\right.
\end{equation}


Note: in the case that $|\mathbf{s}|$ is not a multiple of 64, then the term $\mathbf{s}_{i\dots+64}$ will correctly refer to fewer than 64 elements if it is the final page.

\subsection{Computation of Work Results}\label{sec:computeworkresult}

We now come to the work result computation function $\Xi$. This forms the basis for all utilization of cores on \Jam. It accepts some work-package $\wpX$ for some nominated core $c$ and results in either an error $\error$ or the work result and series of exported segments. This function is deterministic and requires only that it be evaluated within eight epochs of a recently finalized block thanks to the historical lookup functionality. It can thus comfortably be evaluated by any node within the auditing period, even allowing for practicalities of imperfect synchronization.

Formally:
\begin{equation}\label{eq:workresultfunction}
  \!\!\Xi \colon \left\{\begin{aligned}
    (\mathbb{P}, \N_\mathsf{C}) &\to \mathbb{W} \\
    (\wpX, c) &\mapsto \begin{cases}
        \error &\when \mathbf{o} \not\in \Y \\
        \tup{\is{a}{\wpX_\wp¬authorizer}, \mathbf{o}, \is{x}{\wpX_\wp¬context}, s, \mathbf{r}} \!\!\!\!\!&\otherwise
    \end{cases}
  \end{aligned}\right.\!\!\!\!\!\!\!
\end{equation}

where:
\begin{align*}
  \mathbf{o} &= \Psi_I(\wpX, c) \\
  (\mathbf{r}, \overline{\mathbf{e}}) &= \transpose[
    (\itemtoresult(\wpX_\wp¬workitems[j], r), \mathbf{e})
    \mid
    (r, \mathbf{e}) = \countupexports(\wpX, j),
    j \orderedin \N_{|\wpX_\wp¬workitems|}
  ] \\
  \countupexports(\wpX, j) &\equiv \marshallrefine(
    \wpX,
    \wpX_\wp¬workitems[j],
    \sum_{k < j}\wpX_\wp¬workitems[k]_\wi¬exportsegments
  )\\
  \marshallrefine(\wpX, w, \ell) &\equiv \\
  &\!\!\!\!\!\!\!\Psi_R(
    \wiX_c,
    \wiX_g,
    \wiX_s,
    \mathcal{H}(\wpX),
    \wiX_\mathbf{y},
    \wpX_\wp¬context,
    \wpX_\wp¬authorizer,
    \importsegmentdata(w),
    \extrinsicdata(w),
    \ell
  )
\end{align*}

The definition here is staged over several functions for ease of reading. The first term to be introduced, $\mathbf{o}$ is the authorization output, the result of the Is-Authorized function. The second term, $(\mathbf{r}, \overline{\mathbf{e}})$ is the sequence of results for each of the work-items in the work-package together with all segments exported by each work-item.

The third and forth definition are helper terms for this, with the third $\countupexports$ performing an ordered accumulation (\ie counter) in order to ensure that the Refine function has access to the total number of exports made from the work-package up to the current work-item. The fourth term, $\marshallrefine$, is essentially just a call to the Refine function, marshalling the relevant data from the work-package and work-item.


The above relies on two functions, $\importsegmentdata$ and $\extrinsicdata$ which, respectively, define the import segment data and the extrinsic data for some work-item argument $w$:
\begin{equation}
  \begin{aligned}
    \importsegmentdata(\wiX \in \mathbb{I}) &\equiv [\mathbf{s}[n] \mid (\mathcal{M}(\mathbf{s}), n) \orderedin \wiX_\wi¬importsegments] \\
    \extrinsicdata(\wiX \in \mathbb{I}) &\equiv [\mathbf{d} \mid (\mathcal{H}(\mathbf{d}), |\mathbf{d}|) \orderedin \wiX_\wi¬extrinsics]
  \end{aligned}
\end{equation}

We may then define $s$ as the data availability specification of the package using these two functions together with the yet to be defined \emph{Availability Specifier} function $\newavailabilityspecifier$ (see section \ref{sec:availabiltyspecifier}):
\begin{equation}
  \begin{aligned}
    s &= \newavailabilityspecifier(\mathcal{H}(\wpX), \se(\wpX, \mathbf{x}, \mathbf{i}, \mathbf{j}), \wideparen{\overline{\mathbf{e}}}) \\
    \where \mathbf{x} &= [\extrinsicdata(\wiX) \mid \wiX \orderedin \wpX_\wp¬workitems]\\
    \also \mathbf{i} &= [\importsegmentdata(\wiX) \mid \wiX \orderedin \wpX_\wp¬workitems]\\
    \also \mathbf{j} &= [\var{\mathcal{J}(\mathbf{s}, n)} \mid (\mathcal{M}(\mathbf{s}), n) \orderedin \wiX_\wi¬importsegments, \wiX \orderedin \wpX_\wp¬workitems]
  \end{aligned}
\end{equation}

Note that while $\importsegmentdata$ and $\mathbf{j}$ are both formulated using the term $\mathbf{s}$ (all segments exported by all work-packages exporting a segment to be imported) such a vast amount of data is not generally needed as the justification can be derived through a single paged-proof. This reduces the worst case data fetching for a guarantor to two segments for every one to be imported. In the case that contiguously exported segments are imported (which we might assume is a fairly common situation), then a single proof-page should be sufficient to justify many imported segments.

Also of note is the lack of length prefixes: only the Merkle paths for the justifications (\ie the elements of $\mathbf{j}$) have a length prefix. All other sequence lengths are determinable through the work package itself.

The Is-Authorized logic it references is first executed to ensure that the work-package warrants the needed core-time. Next, the guarantor should ensure that all segment-tree roots which form imported segment commitments are known and have not expired. Finally, the guarantor should ensure that they can fetch all preimage data referenced as the commitments of extrinsic segments.

Once done, then imported segments must be reconstructed. This process may in fact be lazy as the Refine function makes no usage of the data until the \emph{import} host-call is made. Fetching generally implies that, for each imported segment, erasure-coded chunks are retrieved from enough unique validators (342, including the guarantor) and is described in more depth in appendix \ref{sec:erasurecoding}. (Since we specify systematic erasure-coding, its reconstruction is trivial in the case that the correct 342 validators are responsive.) Chunks must be fetched for both the data itself and for justification metadata which allows us to ensure that the data is correct.

Validators, in their role as availability assurers, should index such chunks according to the index of the segments-tree whose reconstruction they facilitate. Since the data for segment chunks is so small at 12 bytes, fixed communications costs should be kept to a bare minimum. A good network protocol (out of scope at present) will allow guarantors to specify only the segments-tree root and index together with a Boolean to indicate whether the proof chunk need be supplied. Since we assume at least 341 other validators are online and benevolent, we can assume that the guarantor can compute $\mathbf{i}$ and $\mathbf{j}$ above with confidence, based on the general availability of data committed to with $\mathbf{s}^\clubsuit$, which is specified below.

%TODO: Make result an error if the number of exported segments > stated number of exports. (If less, then additional ones are assumed to be zeroed.)

\subsubsection{Availability Specifier}\label{sec:availabiltyspecifier}
We define the availability specifier function $\newavailabilityspecifier$, which creates an availability specifier from the package hash, an octet sequence of the audit-friendly work-package bundle (comprising the work-package itself, the extrinsic data and the concatenated import segments along with their proofs of correctness), and the sequence of exported segments:
\begin{equation}
  \!\!\!\newavailabilityspecifier\colon\left\{\,\begin{aligned}
    &\tuple{\H, \Y, \seq{\G}} \to \mathbb{S}\\
    &\tup{h, \mathbf{b},\,\mathbf{s}} \mapsto \tup{
      h,\,
      \is{l}{|\mathbf{b}|},\,
      u,\,
      \is{e}{\mathcal{M}(\mathbf{s})}
    }
  \end{aligned}\right.\!\!\!\!\!
\end{equation}
\begin{align*}
  \where u &= \mathcal{M}_B(
    [\wideparen{\mathbf{x}} \mid \mathbf{x} \in \transpose[\mathbf{b}^\clubsuit, \mathbf{s}^\clubsuit]]
  )\\
  \also \mathbf{b}^\clubsuit &= \mathcal{H}^\#(\mathcal{C}_{\ceil{\nicefrac{|\mathbf{b}|}{\mathsf{W}_C}}}(\mathcal{P}_{\mathsf{W}_C}(\mathbf{b})))\\
  \also \mathbf{s}^\clubsuit &= \mathcal{M}_B^\#(\transpose\mathcal{C}^\#_6(\mathbf{s} \concat \pagedproofs(\mathbf{s})))
\end{align*}

% TODO: \mathbb{S}_h is now the hash of the work-package bundle, not the work-package itself. This is a change from the original document... does anything break with it?

The paged-proofs function $\pagedproofs$, defined earlier in equation \ref{eq:pagedproofs}, accepts a sequence of segments and returns a sequence of paged-proofs sufficient to justify the correctness of every segment. There are exactly $\ceil{\nicefrac{1}{64}}$ paged-proof segments as the number of yielded segments, each composed of a page of 64 hashes of segments, together with a Merkle proof from the root to the subtree-root which includes those 64 segments.

The functions $\mathcal{M}$ and $\mathcal{M}_B$ are the fixed-depth and simple binary Merkle root functions, defined in equations \ref{eq:constantdepthmerkleroot} and \ref{eq:simplemerkleroot}. The function $\mathcal{C}$ is the erasure-coding function, defined in appendix \ref{sec:erasurecoding}.

And $\mathcal{P}$ is the zero-padding function to take an octet array to some multiple of $n$ in length:
\begin{equation}\label{eq:zeropadding}
  \mathcal{P}_{n \in \N_{1\dots}}\colon\left\{\begin{aligned}
    \Y &\to \Y_{k\cdot n}\\
    \mathbf{x} &\mapsto \mathbf{x} \concat [0, 0, ...]_{((|x|+n - 1) \bmod n) + 1 \dots n}
  \end{aligned}\right.
\end{equation}

Validators are incentivized to distribute each newly erasure-coded data chunk to the relevant validator, since they are not paid for guaranteeing unless a work-report is considered to be \emph{available} by a super-majority of validators. Given our work-package $\mathbf{p}$, we should therefore send the corresponding work-package bundle chunk and exported segments chunks to each validator whose keys are together with similarly corresponding chunks for imported, extrinsic and exported segments data, such that each validator can justify completeness according to the work-report's \emph{erasure-root}. In the case of a coming epoch change, they may also maximize expected reward by distributing to the new validator set.

We will see this function utilized in the next sections, for guaranteeing, auditing and judging.

\undef{\newavailabilityspecifier}
\undef{\itemtoresult}
\undef{\countupexports}
\undef{\importsegmentdata}
\undef{\pagedproofs}
\undef{\marshallrefine}
\undef{\extrinsicdata}
\undef{\wpX}
\undef{\wiX}