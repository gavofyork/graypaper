\section{Work Packages and Work Reports}\label{sec:workpackagesandworkreports}

\newcommand*{\newavailabilityspecifier}{A}
\newcommand*{\itemtodigest}{C}
\newcommand*{\countupexports}{I}
\newcommand*{\importsegmentdata}{S}
\newcommand*{\justifysegmentdata}{J}
\newcommand*{\segrootlookup}{L}
\newcommand*{\pagedproofs}{P}
\newcommand*{\marshallrefine}{R}
\newcommand*{\extrinsicdata}{X}
\newcommand*{\wpX}{p}
\newcommand*{\wiX}{w}
\newcommand*{\segsX}{\mathbf{b}}
\newcommand*{\allexports}{\overline{\mathbf{e}}}

\subsection{Honest Behavior}

We have so far specified how to recognize blocks for a correctly transitioning \Jam blockchain. Through defining the state transition function and a state Merklization function, we have also defined how to recognize a valid header. While it is not especially difficult to understand how a new block may be authored for any node which controls a key which would allow the creation of the two signatures in the header, nor indeed to fill in the other header fields, readers will note that the contents of the extrinsic remain unclear.

We define not only correct behavior through the creation of correct blocks but also \emph{honest behavior}, which involves the node taking part in several \emph{off-chain} activities. This does have analogous aspects within \emph{YP} Ethereum, though it is not mentioned so explicitly in said document: the creation of blocks along with the gossiping and inclusion of transactions within those blocks would all count as off-chain activities for which honest behavior is helpful. In \Jam's case, honest behavior is well-defined and expected of at least $\nicefrac{2}{3}$ of validators.

Beyond the production of blocks, incentivized honest behavior includes:
\begin{itemize}
    \item the guaranteeing and reporting of work-packages, along with chunking and distribution of both the chunks and the work-package itself, discussed in section \ref{sec:guaranteeing};
    \item assuring the availability of work-packages after being in receipt of their data;
    \item determining which work-reports to audit, fetching and auditing them, and creating and distributing judgments appropriately based on the outcome of the audit;
    \item submitting the correct amount of auditing work seen being done by other validators, discussed in section \ref{sec:bookkeeping}.
\end{itemize}

\subsection{Segments and the Manifest}

Our basic erasure-coding segment size is $\mathsf{W}_E = 684$ octets, derived from the fact we wish to be able to reconstruct even should almost two-thirds of our 1023 participants be malicious or incapacitated, the 16-bit Galois field on which the erasure-code is based and the desire to efficiently support encoding data of close to, but no less than, 4\textsc{kb}.

Work-packages are generally small to ensure guarantors need not invest a lot of bandwidth in order to discover whether they can get paid for their evaluation into a work-report. Rather than having much data inline, they instead \emph{reference} data through commitments. The simplest commitments are extrinsic data.

Extrinsic data are blobs which are being introduced into the system alongside the work-package itself generally by the work-package builder. They are exposed to the Refine logic as an argument. We commit to them through including each of their hashes in the work-package.

Work-packages have two other types of external data associated with them: A cryptographic commitment to each \emph{imported} segment and finally the number of segments which are \emph{exported}.

\subsubsection{Segments, Imports and Exports}

The ability to communicate large amounts of data from one work-package to some subsequent work-package is a key feature of the \Jam availability system. An export segment, defined as the set $\G$, is an octet sequence of fixed length $\mathsf{W}_G = 4104$. It is the smallest datum which may individually be imported from---or exported to---the long-term D$^3$L during the Refine function of a work-package. Being an exact multiple of the erasure-coding piece size ensures that the data segments of work-package can be efficiently placed in the D$^3$L system.
\begin{equation}\label{eq:segment}
  \G \equiv \Y_{\mathsf{W}_G}
\end{equation}

Exported segments are data which are \emph{generated} through the execution of the Refine logic and thus are a side effect of transforming the work-package into a work-report. Since their data is deterministic based on the execution of the Refine logic, we do not require any particular commitment to them in the work-package beyond knowing how many are associated with each Refine invocation in order that we can supply an exact index.

On the other hand, imported segments are segments which were exported by previous work-packages. In order for them to be easily fetched and verified they are referenced not by hash but rather the root of a Merkle tree which includes any other segments introduced at the time, together with an index into this sequence. This allows for justifications of correctness to be generated, stored, included alongside the fetched data and verified. This is described in depth in the next section.

\subsubsection{Data Collection and Justification}

% TODO: Mention core affinity and guarantor-guarantor distribution.

It is the task of a guarantor to reconstitute all imported segments through fetching said segments' erasure-coded chunks from enough unique validators. Reconstitution alone is not enough since corruption of the data would occur if one or more validators provided an incorrect chunk. For this reason we ensure that the import segment specification (a Merkle root and an index into the tree) be a kind of cryptographic commitment capable of having a justification applied to demonstrate that any particular segment is indeed correct.

Justification data must be available to any node over the course of its segment's potential requirement. At around 350 bytes to justify a single segment, justification data is too voluminous to have all validators store all data. We therefore use the same overall availability framework for hosting justification metadata as the data itself.

The guarantor is able to use this proof to justify to themselves that they are not wasting their time on incorrect behavior. We do not force auditors to go through the same process. Instead, guarantors build an \emph{Auditable Work Package}, and place this in the Audit \textsc{da} system. This is the original work-package, its extrinsic data, its imported data and a concise proof of correctness of that imported data. This tactic routinely duplicates data between the D$^3$L and the Audit \textsc{da}, however it is acceptable in order to reduce the bandwidth cost for auditors who must justify the correctness as cheaply as possible as auditing happens on average 30 times for each work-package whereas guaranteeing happens only twice or thrice.

\subsection{Packages and Items}\label{sec:packagesanditems}

We begin by defining a \emph{work-package}, of set $\mathbb{P}$, and its constituent \emph{work-item}s, of set $\mathbb{I}$. A work-package includes a simple blob acting as an authorization token $\wp¬authtoken$, the index of the service which hosts the authorization code $\wp¬authcodehost$, an authorization code hash $\wp¬authcodehash$ and a configuration blob $\wp¬authconfig$, a context $\wp¬context$ and a sequence of work items $\wp¬workitems$:
\begin{equation}\label{eq:workpackage}
  \mathbb{P} \equiv \tuple{
    \isa{\wp¬authtoken}{\Y},\ 
    \isa{\wp¬authcodehost}{\N_S},\ 
    \isa{\wp¬authcodehash}{\H},\ 
    \isa{\wp¬authconfig}{\Y},\ 
    \isa{\wp¬context}{\mathbb{X}},\ 
    \isa{\wp¬workitems}{\seq{\mathbb{I}}_{1:\mathsf{I}}}
  }
\end{equation}

A work item includes: $\wi¬service$ the identifier of the service to which it relates, the code hash of the service at the time of reporting $\wi¬codehash$ (whose preimage must be available from the perspective of the lookup anchor block), a payload blob $\wi¬payload$, gas limits for Refinement and Accumulation $\wi¬refgaslimit$ \& $\wi¬accgaslimit$, and the three elements of its manifest, a sequence of imported data segments $\wi¬importsegments$ which identify a prior exported segment through an index and the identity of an exporting work-package, $\wi¬extrinsics$, a sequence of blob hashes and lengths to be introduced in this block (and which we assume the validator knows) and $\wi¬exportcount$ the number of data segments exported by this work item.
\begin{equation}\label{eq:workitem}
  \mathbb{I} \equiv \tuple{\begin{aligned}
    &\isa{\wi¬service}{\N_S},
    \isa{\wi¬codehash}{\H},
    \isa{\wi¬payload}{\Y},
    \isa{\wi¬refgaslimit}{\N_G},
    \isa{\wi¬accgaslimit}{\N_G},
    \isa{\wi¬exportcount}{\N}, \\
    &\isa{\wi¬importsegments}{\seq{\tuple{\H \cup (\H^\boxplus),\N}}},
    \isa{\wi¬extrinsics}{\seq{(\H, \N)}}
  \end{aligned}}
\end{equation}

Note that an imported data segment's work-package is identified through the union of sets $\H$ and a tagged variant $\H^\boxplus$. A value drawn from the regular $\H$ implies the hash value is of the segment-root containing the export, whereas a value drawn from $\H^\boxplus$ implies the hash value is the hash of the exporting work-package. In the latter case it must be converted into a segment-root by the guarantor and this conversion reported in the work-report for on-chain validation.

We limit the total number of exported items to $\mathsf{W}_X = 3072$, the total number of imported items to $\mathsf{W}_M = 3072$, and the total number of extrinsics to $\mathsf{T} = 128$:
\begin{equation}
  \!\!\!\!\begin{aligned}
    &\forall \wpX \in \mathbb{P}: \\
    &\ \sum_{\wiX \in \wpX_\wp¬workitems} \wiX_\wi¬exportcount \le \mathsf{W}_X \ \wedge\ 
    \sum_{\wiX \in \wpX_\wp¬workitems} |\wiX_\wi¬importsegments| \le \mathsf{W}_M \ \wedge\ 
    \sum_{\wiX \in \wpX_\wp¬workitems} |\wiX_\wi¬extrinsics| \le \mathsf{T}
  \end{aligned}
\end{equation}

We make an assumption that the preimage to each extrinsic hash in each work-item is known by the guarantor. In general this data will be passed to the guarantor alongside the work-package.

We limit the total size of the implied import and extrinsic items, together with all payloads, the authorizer configuration and the authorization token to 12\textsc{mb} in order to allow for around 2\textsc{mb}/s/core data throughput:
\begin{align}
  \label{eq:checkextractsize}
  &\begin{aligned}
    &\forall \wpX \in \mathbb{P}: \Big(|\wpX_\wp¬authtoken| + |\wpX_\wp¬authconfig| +
    \!\!\sum_{\wiX \in \wpX_\wp¬workitems}\!\!S(\wiX)\Big) \le \mathsf{W}_B \\
    &\where S(\wiX \in \mathbb{I}) \equiv |\wiX_\wi¬payload| + |\wiX_\wi¬importsegments|\cdot\mathsf{W}_G + \!\!\!\!\!\!\sum_{(h, l) \in \wiX_\wi¬extrinsics} \!\!\!l
  \end{aligned}\\
  &\mathsf{W}_B \equiv 12\cdot2^{20}
\end{align}

We limit the sums of each of the two gas limits to be at most the maximum gas allocated to a core for the corresponding operation:
\begin{equation}
  \forall \wpX \in \mathbb{P}:\ \;
    \sum_{\wiX \in \wpX_\wp¬workitems}(\wiX_a) < \mathsf{G}_A
  \quad\wedge\ \;
    \sum_{\wiX \in \wpX_\wp¬workitems}(\wiX_g) < \mathsf{G}_R
\end{equation}

%The implication of equation \ref{eq:checkextractsize} is that we have access to the preimage of all extrinsic data. For guaranteeing, this implies the work-package author probably submits the preimages alongside the work-package itself. For auditing, the extrinsic preimages may be reconstructed in the same manner as the work-package. Both are described later.

Given the result $\wl¬result$ and gas used $\wl¬gasused$ of some work-item, we define the item-to-digest function $\itemtodigest$ as:
\begin{equation}
  \itemtodigest\colon\left\{\begin{aligned}
    (\mathbb{I}, \Y \cup \mathbb{J}, \N_G) &\to \mathbb{L}\\
    \tup{\tup{\begin{aligned}
      &\¬service, \¬codehash, \¬payload,\\
      &\wi¬accgaslimit, \¬exportcount, \wi¬importsegments, \wi¬extrinsics
    \end{aligned}
    }, \wl¬result, \wl¬gasused} &\mapsto \tup{\begin{aligned}
      &\wl¬service,\,
      \wl¬codehash,\,
      \is{\wl¬payloadhash}{\hash(\¬payload)},\,
      \is{\wl¬gaslimit}{\wi¬accgaslimit},\,
      \wl¬result,\,
      \wl¬gasused,\\
      &\is{\wl¬importcount}{|\wi¬importsegments|},\,
      \wl¬exportcount,\,
      \is{\wl¬xtcount}{|\wi¬extrinsics|},\,
      \is{\wl¬xtsize}{\!\!\!\!\sum_{(h, z) \in \wi¬extrinsics}\!\!\!\!z}
    \end{aligned}}\!\!\!\!
  \end{aligned}\right.
\end{equation}

We define the work-package's implied authorizer as $\wpX_\wp¬authorizer$, the hash of the authorization code hash concatenated with the configuration. We define the authorization code as $\wpX_\wp¬authcode$ and require that it be available at the time of the lookup anchor block from the historical lookup of service $\wpX_\wp¬authcodehost$. Formally:
\begin{equation}
  \forall \wpX \in \mathbb{P}: \left\{\,\begin{aligned}
    \wpX_\wp¬authorizer &\equiv \hash(\wpX_\wp¬authcodehash \concat \wpX_\wp¬authconfig) \\
    \se(\var{\wpX_\wp¬metadata}, \wpX_\wp¬authcode) &\equiv \Lambda(\delta[\wpX_\wp¬authcodehost], (\wpX_\wp¬context)_t, \wpX_\wp¬authcodehash) \\
    (\wpX_\wp¬metadata, \wpX_\wp¬authcode) &\in (\Y, \Y)
  \end{aligned}\right.
\end{equation}

(The historical lookup function, $\Lambda$, is defined in equation \ref{eq:historicallookup}.)

\subsubsection{Exporting}
Any of a work-package's work-items may \emph{export} segments and a \emph{segments-root} is placed in the work-report committing to these, ordered according to the work-item which is exporting. It is formed as the root of a constant-depth binary Merkle tree as defined in equation \ref{eq:constantdepthmerkleroot}.

Guarantors are required to erasure-code and distribute two data sets: one blob, the auditable \emph{bundle} containing the encoded work-package, extrinsic data and self-justifying imported segments which is placed in the short-term Audit \textsc{da} store; and a second set of exported-segments data together with the \emph{Paged-Proofs} metadata. Items in the first store are short-lived; assurers are expected to keep them only until finality of the block in which the availability of the work-result's work-package is assured. Items in the second, meanwhile, are long-lived and expected to be kept for a minimum of 28 days (672 complete epochs) following the reporting of the work-report. This latter store is referred to as the \emph{Distributed, Decentralized, Data Lake} or D$^3$L owing to its large size.

We define the paged-proofs function $\pagedproofs$ which accepts a series of exported segments $\mathbf{s}$ and defines some series of additional segments placed into the D$^3$L via erasure-coding and distribution. The function evaluates to pages of hashes, together with subtree proofs, such that justifications of correctness based on a segments-root may be made from it:
\begin{equation}\label{eq:pagedproofs}
  \!\!\pagedproofs\colon\left\{\begin{aligned}
    \seq{\G} \to \,&\seq{\G} \\
    \mathbf{s} \mapsto \,&\sq{
      \mathcal{P}_l(\se(
        \var{\mathcal{J}_6(\mathbf{s}, i)},
        \var{\mathcal{L}_6(\mathbf{s}, i)}
      ))
      \mid i \orderedin \N_{\ceil{\nicefrac{|\mathbf{s}|}{64}}}
    } \\
    & \where l = \mathsf{W}_G
  \end{aligned}\right.\!\!\!\!
\end{equation}

\subsection{Computation of Work-Report}\label{sec:computeworkreport}

We now come to the work-report computation function $\Xi$. This forms the basis for all utilization of cores on \Jam. It accepts some work-package $\wpX$ for some nominated core $\¬core$ and results in either an error $\error$ or the work-report and series of exported segments. This function is deterministic and requires only that it be evaluated within eight epochs of a recently finalized block thanks to the historical lookup functionality. It can thus comfortably be evaluated by any node within the auditing period, even allowing for practicalities of imperfect synchronization. Formally:
\begin{equation}\label{eq:workdigestfunction}
  \Xi \colon \left\{\begin{aligned}
    (\mathbb{P}, \N_\mathsf{C}) &\to \mathbb{W} \\
    (\wpX, \¬core) &\mapsto \begin{cases}
        \error &\when \wr¬authtrace \not\in \Y_{:\mathsf{W}_R} \\
        \tup{\wr¬avspec, \is{\wr¬context}{\wpX_\wp¬context}, \¬core, \is{\wr¬authorizer}{\wpX_\wp¬authorizer}, \wr¬authtrace, \wr¬srlookup, \wr¬digests, \wr¬authgasused} &\otherwise
    \end{cases}
  \end{aligned}\right.
\end{equation}
% TODO: Decide on whether context is bold or not.

Where:
\begin{align*}
  \keys{\wr¬srlookup} \equiv \,&\{ h \mid \wiX \in \wpX_\mathbf{w}, (h^\boxplus, n) \in \wiX_\mathbf{i} \} \ ,\quad|\wr¬srlookup| \le 8\\
  (\wr¬authtrace, \wr¬authgasused) = \,&\Psi_I(\wpX, \¬core) \\
  (\wr¬digests, \allexports) = \,&\transpose[
    (\itemtodigest(\wpX_\wp¬workitems[j], r, u), \mathbf{e})
    \mid
    (r, u, \mathbf{e}) = \countupexports(\wpX, j),\,
    j \orderedin \N_{|\wpX_\wp¬workitems|}
  ] \\
  \countupexports(\wpX, j) \equiv \,&\begin{cases}
    (\oversize, u, [\mathbb{G}_0, \mathbb{G}_0, \dots]_{\dots w_e}) &\when |r| + z < \mathsf{W}_R\\
    (\badexports, u, [\mathbb{G}_0, \mathbb{G}_0, \dots]_{\dots w_e}) &\otherwhen |\mathbf{e}| \ne w_e \\
    (r, u, [\mathbb{G}_0, \mathbb{G}_0, \dots]_{\dots w_e}) &\otherwhen r \not\in \mathbb{Y} \\
    (r, u, \mathbf{e}) &\otherwise \\
    \multicolumn{2}{l}{\where (r, \mathbf{e}, u) = \Psi_R(
      j, \wpX, \mathbf{o}, \importsegmentdata(w), \ell
    )}\\
    \multicolumn{2}{l}{\also h = \hash(\wpX)\,,\; w = \wpX_\wp¬workitems[j]\,,\; \ell = \sum_{k < j}\wpX_\wp¬workitems[k]_\wi¬exportcount}\\
    \multicolumn{2}{l}{\also z = |\mathbf{o}| + \sum_{k < j, (r \in \Y, \dots) = I(p, k)} |r|}
  \end{cases}
\end{align*}

% TODO: Handle case where there's an output size overrun gracefully too.

Note that we gracefully handle both the case where the output size of the work output would take the work-report beyond its acceptable size and where number of segments exported by a work-item's Refinement execution is incorrectly reported in the work-item's export segment count. In both cases, the work-package continues to be valid as a whole, but the work-item's exported segments are replaced by a sequence of zero-segments equal in size to the export segment count and its output is replaced by an error.

Initially we constrain the segment-root dictionary $\wr¬srlookup$: It should contain entries for all unique work-package hashes of imported segments not identified directly via a segment-root but rather through a work-package hash.

We immediately define the segment-root lookup function $\segrootlookup$, dependent on this dictionary, which collapses a union of segment-roots and work-package hashes into segment-roots using the dictionary:
\begin{equation}
  \segrootlookup(r \in \H\cup\H^\boxplus) \equiv \begin{cases}
    r &\when r \in \H \\
    \wr¬srlookup[h] &\when \exists h \in \H: r = h^\boxplus
  \end{cases}
\end{equation}

In order to expect to be compensated for a work-report they are building, guarantors must compose a value for $\wr¬srlookup$ to ensure not only the above but also a further constraint that all pairs of work-package hashes and segment-roots do properly correspond:
\begin{equation}
  \forall (h \mapsto e) \in \wr¬srlookup : \exists \wpX, \¬core \in \mathbb{P}, \N_\mathsf{C} : \hash(\wpX) = h \wedge (\Xi(\wpX, \¬core)_s)_e = e\!\!\!\!
\end{equation}

As long as the guarantor is unable to satisfy the above constraints, then it should consider the work-package unable to be guaranteed. Auditors are not expected to populate this but rather to reuse the value in the work-report they are auditing.

The next term to be introduced, $\tup{\wr¬authtrace, \wr¬authgasused}$, is the authorization trace, the result of the Is-Authorized function together with the amount of gas it used. The second term, $\tup{\wr¬digests, \allexports}$ is the sequence of results for each of the work-items in the work-package together with all segments exported by each work-item. The third definition $\countupexports$ performs an ordered accumulation (\ie counter) in order to ensure that the Refine function has access to the total number of exports made from the work-package up to the current work-item.

The above relies on two functions, $\importsegmentdata$ and $\extrinsicdata$ which, respectively, define the import segment data and the extrinsic data for some work-item argument $\wiX$. We also define $\justifysegmentdata$, which compiles justifications of segment data:
\begin{equation}
  \begin{aligned}
    \extrinsicdata(\wiX \in \mathbb{I}) &\equiv [\mathbf{d} \mid (\hash(\mathbf{d}), |\mathbf{d}|) \orderedin \wiX_\wi¬extrinsics] \\
    \importsegmentdata(\wiX \in \mathbb{I}) &\equiv [\segsX[n] \mid \mathcal{M}(\segsX) = \segrootlookup(r), (r, n) \orderedin \wiX_\wi¬importsegments] \\
    \justifysegmentdata(\wiX \in \mathbb{I}) &\equiv [\var{\mathcal{J}_0(\segsX, n)} \mid \mathcal{M}(\segsX) = \segrootlookup(r), (r, n) \orderedin \wiX_\wi¬importsegments]
  \end{aligned}
\end{equation}

We may then define $\wr¬avspec$ as the data availability specification of the package using these two functions together with the yet to be defined \emph{Availability Specifier} function $\newavailabilityspecifier$ (see section \ref{sec:availabiltyspecifier}):
\begin{equation}
  \wr¬avspec = \newavailabilityspecifier(
    \hash(\wpX),
    \se(
      \wpX,
      \extrinsicdata^\#(\wpX_\wp¬workitems),
      \importsegmentdata^\#(\wpX_\wp¬workitems),
      \justifysegmentdata^\#(\wpX_\wp¬workitems)
    ),
    \wideparen{\allexports}
  )\!\!\!\!
\end{equation}

Note that while the formulations of $\importsegmentdata$ and $\justifysegmentdata$ seem to require (due to the inner term $\segsX$) all segments exported by all work-packages exporting a segment to be imported, such a vast amount of data is not generally needed. In particular, each justification can be derived through a single paged-proof. This reduces the worst case data fetching for a guarantor to two segments for every one to be imported. In the case that contiguously exported segments are imported (which we might assume is a fairly common situation), then a single proof-page should be sufficient to justify many imported segments.

Also of note is the lack of length prefixes: only the Merkle paths for the justifications have a length prefix. All other sequence lengths are determinable through the work package itself.

The Is-Authorized logic it references must be executed first in order to ensure that the work-package warrants the needed core-time. Next, the guarantor should ensure that all segment-tree roots which form imported segment commitments are known and have not expired. Finally, the guarantor should ensure that they can fetch all preimage data referenced as the commitments of extrinsic segments.

Once done, then imported segments must be reconstructed. This process may in fact be lazy as the Refine function makes no usage of the data until the \emph{fetch} host-call is made. Fetching generally implies that, for each imported segment, erasure-coded chunks are retrieved from enough unique validators (342, including the guarantor) and is described in more depth in appendix \ref{sec:erasurecoding}. (Since we specify systematic erasure-coding, its reconstruction is trivial in the case that the correct 342 validators are responsive.) Chunks must be fetched for both the data itself and for justification metadata which allows us to ensure that the data is correct.

Validators, in their role as availability assurers, should index such chunks according to the index of the segments-tree whose reconstruction they facilitate. Since the data for segment chunks is so small at 12 octets, fixed communications costs should be kept to a bare minimum. A good network protocol (out of scope at present) will allow guarantors to specify only the segments-tree root and index together with a Boolean to indicate whether the proof chunk need be supplied. Since we assume at least 341 other validators are online and benevolent, we can assume that the guarantor can compute $\importsegmentdata$ and $\justifysegmentdata$ above with confidence, based on the general availability of data committed to with $\mathbf{s}^\clubsuit$, which is specified below.

%TODO: Make result an error if the number of exported segments > stated number of exports. (If less, then additional ones are assumed to be zeroed.)

\subsubsection{Availability Specifier}\label{sec:availabiltyspecifier}
We define the availability specifier function $\newavailabilityspecifier$, which creates an availability specifier from the package hash, an octet sequence of the audit-friendly work-package bundle (comprising the work-package itself, the extrinsic data and the concatenated import segments along with their proofs of correctness), and the sequence of exported segments:
\begin{equation}
  \!\!\!\newavailabilityspecifier\colon\left\{\,\begin{aligned}
    \tuple{\H, \Y, \seq{\G}} &\to \avspec\\
    \tup{h, \mathbf{b},\,\mathbf{s}} &\mapsto \tup{
      h,\,
      \is{l}{|\mathbf{b}|},\,
      u,\,
      \is{e}{\mathcal{M}(\mathbf{s})},\,
      \is{n}{|\mathbf{s}|}
    }
  \end{aligned}\right.\!\!\!\!\!
\end{equation}
\begin{align*}
  \where u &= \mathcal{M}_B(
    [\wideparen{\mathbf{x}} \mid \mathbf{x} \orderedin \transpose[\mathbf{b}^\clubsuit, \mathbf{s}^\clubsuit]]
  )\\
  \also \mathbf{b}^\clubsuit &= \hash^\#(\mathcal{C}_{\ceil{\nicefrac{|\mathbf{b}|}{\mathsf{W}_E}}}(\mathcal{P}_{\mathsf{W}_E}(\mathbf{b})))\\
  \also \mathbf{s}^\clubsuit &= \mathcal{M}_B^\#(\transpose\mathcal{C}^\#_6(\mathbf{s} \concat \pagedproofs(\mathbf{s})))
\end{align*}

% TODO: \avspec_h is now the hash of the work-package bundle, not the work-package itself. This is a change from the original document... does anything break with it?

The paged-proofs function $\pagedproofs$, defined earlier in equation \ref{eq:pagedproofs}, accepts a sequence of segments and returns a sequence of paged-proofs sufficient to justify the correctness of every segment. There are exactly $\ceil{\nicefrac{1}{64}}$ paged-proof segments as the number of yielded segments, each composed of a page of 64 hashes of segments, together with a Merkle proof from the root to the subtree-root which includes those 64 segments.

The functions $\mathcal{M}$ and $\mathcal{M}_B$ are the fixed-depth and simple binary Merkle root functions, defined in equations \ref{eq:constantdepthmerkleroot} and \ref{eq:simplemerkleroot}. The function $\mathcal{C}$ is the erasure-coding function, defined in appendix \ref{sec:erasurecoding}.

And $\mathcal{P}$ is the zero-padding function to take an octet array to some multiple of $n$ in length:
\begin{equation}\label{eq:zeropadding}
  \mathcal{P}_{n \in \N_{1\dots}}\colon\left\{\begin{aligned}
    \Y &\to \Y_{k\cdot n}\\
    \mathbf{x} &\mapsto \mathbf{x} \concat [0, 0, ...]_{((|x|+n - 1) \bmod n) + 1 \dots n}
  \end{aligned}\right.
\end{equation}

Validators are incentivized to distribute each newly erasure-coded data chunk to the relevant validator, since they are not paid for guaranteeing unless a work-report is considered to be \emph{available} by a super-majority of validators. Given our work-package $\mathbf{p}$, we should therefore send the corresponding work-package bundle chunk and exported segments chunks to each validator whose keys are together with similarly corresponding chunks for imported, extrinsic and exported segments data, such that each validator can justify completeness according to the work-report's \emph{erasure-root}. In the case of a coming epoch change, they may also maximize expected reward by distributing to the new validator set.

We will see this function utilized in the next sections, for guaranteeing, auditing and judging.

\undef{\newavailabilityspecifier}
\undef{\itemtodigest}
\undef{\countupexports}
\undef{\importsegmentdata}
\undef{\pagedproofs}
\undef{\marshallrefine}
\undef{\extrinsicdata}
\undef{\wpX}
\undef{\wiX}